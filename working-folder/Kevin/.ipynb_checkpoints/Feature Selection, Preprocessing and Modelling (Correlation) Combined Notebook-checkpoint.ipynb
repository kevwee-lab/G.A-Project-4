{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import  XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading combined dataset\n",
    "\n",
    "data = pd.read_csv('../../data/combined_kev.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "data.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for duplicates\n",
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sunrise & sunset to datetime\n",
    "sunrise = pd.to_datetime(data['sunrise'], format='%H%M')\n",
    "\n",
    "# fix entries - round off 60min to the next hour\n",
    "sunset = pd.to_datetime(data['sunset'].map(lambda x: 1800 if x == 1760 else x), format='%H%M')\n",
    "\n",
    "# convert to seconds from start of day to be consistent input into model\n",
    "data['sunrise'] = (sunrise - sunrise.dt.normalize()).dt.seconds\n",
    "data['sunset'] = (sunset - sunset.dt.normalize()).dt.seconds\n",
    "\n",
    "# add total sunlight time feature\n",
    "data['total_sunlight_time'] = (sunset - sunrise).dt.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>species</th>\n",
       "      <th>wnvpresent</th>\n",
       "      <th>trap</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_ord</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tavg</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_humid_lag14</th>\n",
       "      <th>rel_humid_lag28</th>\n",
       "      <th>avgspeed_lag5</th>\n",
       "      <th>avgspeed_lag14</th>\n",
       "      <th>avgspeed_lag28</th>\n",
       "      <th>preciptotal_lag5</th>\n",
       "      <th>preciptotal_lag14</th>\n",
       "      <th>preciptotal_lag28</th>\n",
       "      <th>week_number</th>\n",
       "      <th>total_sunlight_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>CULEX PIPIENS</td>\n",
       "      <td>0</td>\n",
       "      <td>T096</td>\n",
       "      <td>41.731922</td>\n",
       "      <td>-87.677512</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>40.692805</td>\n",
       "      <td>38.885597</td>\n",
       "      <td>7.51</td>\n",
       "      <td>9.682143</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069286</td>\n",
       "      <td>0.055357</td>\n",
       "      <td>22</td>\n",
       "      <td>53760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>CULEX PIPIENS/RESTUANS</td>\n",
       "      <td>0</td>\n",
       "      <td>T086</td>\n",
       "      <td>41.688324</td>\n",
       "      <td>-87.676709</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>40.692805</td>\n",
       "      <td>38.885597</td>\n",
       "      <td>7.51</td>\n",
       "      <td>9.682143</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069286</td>\n",
       "      <td>0.055357</td>\n",
       "      <td>22</td>\n",
       "      <td>53760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>CULEX PIPIENS/RESTUANS</td>\n",
       "      <td>0</td>\n",
       "      <td>T048</td>\n",
       "      <td>41.867108</td>\n",
       "      <td>-87.654224</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>40.692805</td>\n",
       "      <td>38.885597</td>\n",
       "      <td>7.51</td>\n",
       "      <td>9.682143</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069286</td>\n",
       "      <td>0.055357</td>\n",
       "      <td>22</td>\n",
       "      <td>53760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>CULEX PIPIENS/RESTUANS</td>\n",
       "      <td>0</td>\n",
       "      <td>T129</td>\n",
       "      <td>41.891126</td>\n",
       "      <td>-87.611560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>40.692805</td>\n",
       "      <td>38.885597</td>\n",
       "      <td>7.51</td>\n",
       "      <td>9.682143</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069286</td>\n",
       "      <td>0.055357</td>\n",
       "      <td>22</td>\n",
       "      <td>53760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>CULEX PIPIENS/RESTUANS</td>\n",
       "      <td>0</td>\n",
       "      <td>T050</td>\n",
       "      <td>41.919343</td>\n",
       "      <td>-87.694259</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>40.692805</td>\n",
       "      <td>38.885597</td>\n",
       "      <td>7.51</td>\n",
       "      <td>9.682143</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.069286</td>\n",
       "      <td>0.055357</td>\n",
       "      <td>22</td>\n",
       "      <td>53760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                 species  wnvpresent  trap   latitude  longitude  \\\n",
       "0 2007-05-29           CULEX PIPIENS           0  T096  41.731922 -87.677512   \n",
       "1 2007-05-29  CULEX PIPIENS/RESTUANS           0  T086  41.688324 -87.676709   \n",
       "2 2007-05-29  CULEX PIPIENS/RESTUANS           0  T048  41.867108 -87.654224   \n",
       "3 2007-05-29  CULEX PIPIENS/RESTUANS           0  T129  41.891126 -87.611560   \n",
       "4 2007-05-29  CULEX PIPIENS/RESTUANS           0  T050  41.919343 -87.694259   \n",
       "\n",
       "   species_ord  tmax  tmin  tavg  ...  rel_humid_lag14  rel_humid_lag28  \\\n",
       "0          2.0    88    62    75  ...        40.692805        38.885597   \n",
       "1          2.0    88    62    75  ...        40.692805        38.885597   \n",
       "2          2.0    88    62    75  ...        40.692805        38.885597   \n",
       "3          2.0    88    62    75  ...        40.692805        38.885597   \n",
       "4          2.0    88    62    75  ...        40.692805        38.885597   \n",
       "\n",
       "   avgspeed_lag5  avgspeed_lag14  avgspeed_lag28  preciptotal_lag5  \\\n",
       "0           7.51        9.682143            10.2              0.15   \n",
       "1           7.51        9.682143            10.2              0.15   \n",
       "2           7.51        9.682143            10.2              0.15   \n",
       "3           7.51        9.682143            10.2              0.15   \n",
       "4           7.51        9.682143            10.2              0.15   \n",
       "\n",
       "   preciptotal_lag14  preciptotal_lag28  week_number  total_sunlight_time  \n",
       "0           0.069286           0.055357           22                53760  \n",
       "1           0.069286           0.055357           22                53760  \n",
       "2           0.069286           0.055357           22                53760  \n",
       "3           0.069286           0.055357           22                53760  \n",
       "4           0.069286           0.055357           22                53760  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking finalised dataset before modelling\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and Preprocessing Workflow\n",
    "As we have decided to optimise for accuracy (ROC-AUC) instead of opting for more interpretability, we will use:\n",
    "\n",
    "**Preprocessing**\n",
    "\n",
    "1) Polynomial features (degree of 2) to increase the number of features. <br>\n",
    "2) Train-test-split <br>\n",
    "3) Standard Scaler <br>\n",
    "\n",
    "**Feature Selection**\n",
    "\n",
    "We will use Pearson's correlation with 4 different cutoff points for correlation (0, 0.01, 0.05, 0.1). This will help us gradually reduce the number of features until we find a feature set that gives us the best test ROC-AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring X and y variables\n",
    "X = data.drop(columns=['date', 'species', 'trap', 'wnvpresent'])\n",
    "y = data['wnvpresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8475, 1274)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#polynomial features the independent variables\n",
    "poly = PolynomialFeatures(include_bias=False, degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making polynomial features into a dataframe\n",
    "X_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly,\n",
    "                                                    y, \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard scale the X_train and X_test variables\n",
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find the best parameters using GridSearch on these models:\n",
    "\n",
    "1) **Support Vector Classifier**: Effective in high dimensional spaces. Uses a subset of training points in the decision function.<br>\n",
    "2) **Logistic Regression**: Effective in binary classification. <br>\n",
    "3) **Gradient boost**: Produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. Uses gradient descent algorithm.<br>\n",
    "4) **Xg boost**: Similar to gradient boost, but xgboost uses a more regularized model formalization to control over-fitting.<br>\n",
    "5) **Random Forest**: Constructs decision trees at training time and outputting the class that is the mode of the classes.<br> \n",
    "6) **Extra Trees**: Similar to Random Forest. However, the splits of the trees in the Random Forest are deterministic. It is random for extratrees. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for logistic regression\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    max_iter=1000, \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "logreg_params = {\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [0.1, 1, 1.5, 2.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gb_params = {\n",
    "    'clf__learning_rate': [0.05, 0.1],\n",
    "    'clf__max_depth': [2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "\n",
    "svc_params = {\n",
    "    'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'clf__C': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xg boost\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, objective='binary:logistic', eval_metric='auc', random_state=42)\n",
    "\n",
    "xgb_params = {\n",
    "    'clf__max_depth': [3, 4, 5],\n",
    "    'clf__gamma' : [ 0.0, 0.2 , 0.4 ],\n",
    "    'clf__eta' : [0.05, 0.15, 0.25]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'clf__max_depth': [3, 4, 5],\n",
    "    'clf__n_estimators': [80, 100, 150],\n",
    "    'clf__min_samples_split': [2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra trees\n",
    "et = ExtraTreesClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "et_params = {\n",
    "    'clf__max_depth': [3, 4, 5],\n",
    "    'clf__n_estimators': [80, 100, 150],\n",
    "    'clf__min_samples_split': [2, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by correlation to y\n",
    "corr_df = X_poly.join(y).corr()[['wnvpresent']].abs().sort_values(by='wnvpresent', ascending=False)\n",
    "\n",
    "# Instantiate result list\n",
    "results = []\n",
    "\n",
    "#create a function to gridsearch all models\n",
    "\n",
    "def model(clf, clf_params, cutoff):\n",
    "    \n",
    "    # Feature list with correlation to y > cutoff value\n",
    "    features = corr_df[corr_df['wnvpresent'] > cutoff].index[1:]\n",
    "    \n",
    "    # Instantiate pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('ss', StandardScaler()),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Gridsearch for best estimator\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=clf_params,\n",
    "        scoring='roc_auc',\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train[features], y_train)\n",
    "    \n",
    "    print(f'Classifier: {clf}, Cutoff value: {cutoff}')\n",
    "    print('Best Parameters:')\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    # Scoring metrics\n",
    "    scores = {'Classifier': clf, 'Cutoff': cutoff}\n",
    "    y_preds = grid.predict(X_test[features])\n",
    "    y_pred_probas = grid.predict_proba(X_test[features])[:, 1]\n",
    "    y_train_pred_probas = grid.predict_proba(X_train[features])[:, 1]\n",
    "    scores['Train ROC-AUC Score'] = metrics.roc_auc_score(y_train, y_train_pred_probas)\n",
    "    scores['Test ROC-AUC Score'] = metrics.roc_auc_score(y_test, y_pred_probas)\n",
    "    scores['F1'] = metrics.f1_score(y_test, y_preds)\n",
    "    scores['Precision'] = metrics.precision_score(y_test, y_preds)\n",
    "    scores['Recall'] = metrics.recall_score(y_test, y_preds)\n",
    "    scores['Accuracy'] = metrics.accuracy_score(y_test, y_preds)\n",
    "    \n",
    "    # Storing results\n",
    "    results.append(scores)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of classifiers and params to fit into function\n",
    "\n",
    "classifiers = [\n",
    "    (logreg, logreg_params),\n",
    "    (gb, gb_params),\n",
    "    (svc, svc_params),\n",
    "    (xgb, xgb_params),\n",
    "    (rf, rf_params),\n",
    "    (et, et_params),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Classifier: LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'), Cutoff value: 0\n",
      "Best Parameters:\n",
      "{'clf__C': 1.5, 'clf__penalty': 'l1'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Classifier: GradientBoostingClassifier(random_state=42), Cutoff value: 0\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.1, 'clf__max_depth': 2}\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Classifier: SVC(probability=True, random_state=42), Cutoff value: 0\n",
      "Best Parameters:\n",
      "{'clf__C': 5, 'clf__kernel': 'linear'}\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Classifier: XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, eval_metric='auc',\n",
      "              gamma=None, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=None,\n",
      "              max_delta_step=None, max_depth=None, min_child_weight=None,\n",
      "              missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
      "              subsample=None, tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None), Cutoff value: 0\n",
      "Best Parameters:\n",
      "{'clf__eta': 0.05, 'clf__gamma': 0.0, 'clf__max_depth': 5}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: RandomForestClassifier(class_weight='balanced', random_state=42), Cutoff value: 0\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 2, 'clf__n_estimators': 150}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: ExtraTreesClassifier(class_weight='balanced', random_state=42), Cutoff value: 0\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 4, 'clf__n_estimators': 150}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Classifier: LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'), Cutoff value: 0.01\n",
      "Best Parameters:\n",
      "{'clf__C': 2.5, 'clf__penalty': 'l1'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Classifier: GradientBoostingClassifier(random_state=42), Cutoff value: 0.01\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.1, 'clf__max_depth': 2}\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Classifier: SVC(probability=True, random_state=42), Cutoff value: 0.01\n",
      "Best Parameters:\n",
      "{'clf__C': 10, 'clf__kernel': 'linear'}\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Classifier: XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, eval_metric='auc',\n",
      "              gamma=None, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=None,\n",
      "              max_delta_step=None, max_depth=None, min_child_weight=None,\n",
      "              missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
      "              subsample=None, tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None), Cutoff value: 0.01\n",
      "Best Parameters:\n",
      "{'clf__eta': 0.15, 'clf__gamma': 0.0, 'clf__max_depth': 3}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: RandomForestClassifier(class_weight='balanced', random_state=42), Cutoff value: 0.01\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 4, 'clf__n_estimators': 80}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: ExtraTreesClassifier(class_weight='balanced', random_state=42), Cutoff value: 0.01\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 4, 'clf__n_estimators': 100}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Classifier: LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'), Cutoff value: 0.05\n",
      "Best Parameters:\n",
      "{'clf__C': 2.5, 'clf__penalty': 'l2'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Classifier: GradientBoostingClassifier(random_state=42), Cutoff value: 0.05\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.1, 'clf__max_depth': 2}\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Classifier: SVC(probability=True, random_state=42), Cutoff value: 0.05\n",
      "Best Parameters:\n",
      "{'clf__C': 5, 'clf__kernel': 'linear'}\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Classifier: XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, eval_metric='auc',\n",
      "              gamma=None, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=None,\n",
      "              max_delta_step=None, max_depth=None, min_child_weight=None,\n",
      "              missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
      "              subsample=None, tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None), Cutoff value: 0.05\n",
      "Best Parameters:\n",
      "{'clf__eta': 0.05, 'clf__gamma': 0.4, 'clf__max_depth': 5}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: RandomForestClassifier(class_weight='balanced', random_state=42), Cutoff value: 0.05\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 4, 'clf__n_estimators': 150}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: ExtraTreesClassifier(class_weight='balanced', random_state=42), Cutoff value: 0.05\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 4, 'clf__n_estimators': 80}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Classifier: LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'), Cutoff value: 0.1\n",
      "Best Parameters:\n",
      "{'clf__C': 2.5, 'clf__penalty': 'l1'}\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weezs\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GradientBoostingClassifier(random_state=42), Cutoff value: 0.1\n",
      "Best Parameters:\n",
      "{'clf__learning_rate': 0.1, 'clf__max_depth': 3}\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Classifier: SVC(probability=True, random_state=42), Cutoff value: 0.1\n",
      "Best Parameters:\n",
      "{'clf__C': 5, 'clf__kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weezs\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Classifier: XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, eval_metric='auc',\n",
      "              gamma=None, gpu_id=None, importance_type='gain',\n",
      "              interaction_constraints=None, learning_rate=None,\n",
      "              max_delta_step=None, max_depth=None, min_child_weight=None,\n",
      "              missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=42,\n",
      "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
      "              subsample=None, tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None), Cutoff value: 0.1\n",
      "Best Parameters:\n",
      "{'clf__eta': 0.15, 'clf__gamma': 0.2, 'clf__max_depth': 3}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: RandomForestClassifier(class_weight='balanced', random_state=42), Cutoff value: 0.1\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 2, 'clf__n_estimators': 150}\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Classifier: ExtraTreesClassifier(class_weight='balanced', random_state=42), Cutoff value: 0.1\n",
      "Best Parameters:\n",
      "{'clf__max_depth': 5, 'clf__min_samples_split': 4, 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#iterate function through each cutoff point\n",
    "\n",
    "for cutoff in [0, 0.01, 0.05, 0.1]:\n",
    "    for (clf, clf_params) in classifiers:\n",
    "        model(clf, clf_params, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Cutoff</th>\n",
       "      <th>Train ROC-AUC Score</th>\n",
       "      <th>Test ROC-AUC Score</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.939520</td>\n",
       "      <td>0.856590</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.948879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.884891</td>\n",
       "      <td>0.855815</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.944160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.880634</td>\n",
       "      <td>0.854353</td>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.945340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.878991</td>\n",
       "      <td>0.854213</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.945733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.889833</td>\n",
       "      <td>0.852942</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.944947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.891547</td>\n",
       "      <td>0.851727</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.944947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.939217</td>\n",
       "      <td>0.851315</td>\n",
       "      <td>0.123288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.949666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.935214</td>\n",
       "      <td>0.850708</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.949273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.875913</td>\n",
       "      <td>0.849350</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.944554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.928680</td>\n",
       "      <td>0.845004</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.948879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GradientBoostingClassifier(random_state=42)</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.924722</td>\n",
       "      <td>0.841508</td>\n",
       "      <td>0.169697</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.946127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier(class_weight='balanced'...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.881894</td>\n",
       "      <td>0.834635</td>\n",
       "      <td>0.243398</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.740857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>0.832852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier(class_weight='balanced'...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.884357</td>\n",
       "      <td>0.831282</td>\n",
       "      <td>0.246512</td>\n",
       "      <td>0.146611</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.745183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForestClassifier(class_weight='balanced'...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.881228</td>\n",
       "      <td>0.830259</td>\n",
       "      <td>0.244571</td>\n",
       "      <td>0.144986</td>\n",
       "      <td>0.781022</td>\n",
       "      <td>0.740071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForestClassifier(class_weight='balanced'...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.877134</td>\n",
       "      <td>0.828516</td>\n",
       "      <td>0.235033</td>\n",
       "      <td>0.138562</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>0.728667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ExtraTreesClassifier(class_weight='balanced', ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.855755</td>\n",
       "      <td>0.828147</td>\n",
       "      <td>0.222672</td>\n",
       "      <td>0.129260</td>\n",
       "      <td>0.802920</td>\n",
       "      <td>0.697994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ExtraTreesClassifier(class_weight='balanced', ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.853207</td>\n",
       "      <td>0.826852</td>\n",
       "      <td>0.221106</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.802920</td>\n",
       "      <td>0.695242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ExtraTreesClassifier(class_weight='balanced', ...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.847351</td>\n",
       "      <td>0.824440</td>\n",
       "      <td>0.223340</td>\n",
       "      <td>0.129522</td>\n",
       "      <td>0.810219</td>\n",
       "      <td>0.696422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ExtraTreesClassifier(class_weight='balanced', ...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.851873</td>\n",
       "      <td>0.823601</td>\n",
       "      <td>0.223615</td>\n",
       "      <td>0.130488</td>\n",
       "      <td>0.781022</td>\n",
       "      <td>0.707825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SVC(probability=True, random_state=42)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.834632</td>\n",
       "      <td>0.779913</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.946127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SVC(probability=True, random_state=42)</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.830578</td>\n",
       "      <td>0.777698</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.946127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SVC(probability=True, random_state=42)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.790788</td>\n",
       "      <td>0.728548</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.945733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SVC(probability=True, random_state=42)</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.694430</td>\n",
       "      <td>0.681546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Classifier  Cutoff  \\\n",
       "0   XGBClassifier(base_score=None, booster=None, c...    0.01   \n",
       "1         GradientBoostingClassifier(random_state=42)    0.05   \n",
       "2   LogisticRegression(max_iter=1000, random_state...    0.01   \n",
       "3   LogisticRegression(max_iter=1000, random_state...    0.00   \n",
       "4         GradientBoostingClassifier(random_state=42)    0.01   \n",
       "5         GradientBoostingClassifier(random_state=42)    0.00   \n",
       "6   XGBClassifier(base_score=None, booster=None, c...    0.00   \n",
       "7   XGBClassifier(base_score=None, booster=None, c...    0.05   \n",
       "8   LogisticRegression(max_iter=1000, random_state...    0.05   \n",
       "9   XGBClassifier(base_score=None, booster=None, c...    0.10   \n",
       "10        GradientBoostingClassifier(random_state=42)    0.10   \n",
       "11  RandomForestClassifier(class_weight='balanced'...    0.01   \n",
       "12  LogisticRegression(max_iter=1000, random_state...    0.10   \n",
       "13  RandomForestClassifier(class_weight='balanced'...    0.00   \n",
       "14  RandomForestClassifier(class_weight='balanced'...    0.05   \n",
       "15  RandomForestClassifier(class_weight='balanced'...    0.10   \n",
       "16  ExtraTreesClassifier(class_weight='balanced', ...    0.00   \n",
       "17  ExtraTreesClassifier(class_weight='balanced', ...    0.01   \n",
       "18  ExtraTreesClassifier(class_weight='balanced', ...    0.10   \n",
       "19  ExtraTreesClassifier(class_weight='balanced', ...    0.05   \n",
       "20             SVC(probability=True, random_state=42)    0.01   \n",
       "21             SVC(probability=True, random_state=42)    0.00   \n",
       "22             SVC(probability=True, random_state=42)    0.05   \n",
       "23             SVC(probability=True, random_state=42)    0.10   \n",
       "\n",
       "    Train ROC-AUC Score  Test ROC-AUC Score        F1  Precision    Recall  \\\n",
       "0              0.939520            0.856590  0.121622   0.818182  0.065693   \n",
       "1              0.884891            0.855815  0.040541   0.272727  0.021898   \n",
       "2              0.880634            0.854353  0.041379   0.375000  0.021898   \n",
       "3              0.878991            0.854213  0.041667   0.428571  0.021898   \n",
       "4              0.889833            0.852942  0.054054   0.363636  0.029197   \n",
       "5              0.891547            0.851727  0.066667   0.384615  0.036496   \n",
       "6              0.939217            0.851315  0.123288   1.000000  0.065693   \n",
       "7              0.935214            0.850708  0.122449   0.900000  0.065693   \n",
       "8              0.875913            0.849350  0.013986   0.166667  0.007299   \n",
       "9              0.928680            0.845004  0.144737   0.733333  0.080292   \n",
       "10             0.924722            0.841508  0.169697   0.500000  0.102190   \n",
       "11             0.881894            0.834635  0.243398   0.144414  0.773723   \n",
       "12             0.855964            0.832852  0.000000   0.000000  0.000000   \n",
       "13             0.884357            0.831282  0.246512   0.146611  0.773723   \n",
       "14             0.881228            0.830259  0.244571   0.144986  0.781022   \n",
       "15             0.877134            0.828516  0.235033   0.138562  0.773723   \n",
       "16             0.855755            0.828147  0.222672   0.129260  0.802920   \n",
       "17             0.853207            0.826852  0.221106   0.128205  0.802920   \n",
       "18             0.847351            0.824440  0.223340   0.129522  0.810219   \n",
       "19             0.851873            0.823601  0.223615   0.130488  0.781022   \n",
       "20             0.834632            0.779913  0.041958   0.500000  0.021898   \n",
       "21             0.830578            0.777698  0.041958   0.500000  0.021898   \n",
       "22             0.790788            0.728548  0.041667   0.428571  0.021898   \n",
       "23             0.694430            0.681546  0.000000   0.000000  0.000000   \n",
       "\n",
       "    Accuracy  \n",
       "0   0.948879  \n",
       "1   0.944160  \n",
       "2   0.945340  \n",
       "3   0.945733  \n",
       "4   0.944947  \n",
       "5   0.944947  \n",
       "6   0.949666  \n",
       "7   0.949273  \n",
       "8   0.944554  \n",
       "9   0.948879  \n",
       "10  0.946127  \n",
       "11  0.740857  \n",
       "12  0.946127  \n",
       "13  0.745183  \n",
       "14  0.740071  \n",
       "15  0.728667  \n",
       "16  0.697994  \n",
       "17  0.695242  \n",
       "18  0.696422  \n",
       "19  0.707825  \n",
       "20  0.946127  \n",
       "21  0.946127  \n",
       "22  0.945733  \n",
       "23  0.946127  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display results by Test ROC-AUC score\n",
    "\n",
    "pd.DataFrame(results).sort_values(by='Test ROC-AUC Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in terms of test ROC-AUC score, the best model is xgboost classifier at a correlation cutoff of 0.01 with the following best parameters:\n",
    "{'clf__eta': 0.15, 'clf__gamma': 0.0, 'clf__max_depth': 3}. \n",
    "\n",
    "The next best model in terms of test ROC-AUC score would be gradient boosting classifier at a correlation cutoff of 0.05 with the following best parameters:\n",
    "{'clf__learning_rate': 0.1, 'clf__max_depth': 2}.\n",
    "\n",
    "We will now proceed to save our top 2 models to file in case we need to use them in our evaluation later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving top 2 best models from feature selection using correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = XGBClassifier(use_label_encoder=False, objective='binary:logistic', eval_metric='auc', random_state=42, eta=0.15, gamma=0.0, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to file\n",
    "\n",
    "Pkl_Filename = \"best_xgb_model_corr.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(best_xgb_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_model = GradientBoostingClassifier(learning_rate=0.1, max_depth=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to file\n",
    "\n",
    "Pkl_Filename = \"best_gb_model_corr.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(best_gb_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
